{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nmji1trStg4"
   },
   "source": [
    "<html>\n",
    "    <summary></summary>\n",
    "    <p float=\"left\">\n",
    "         <div> <p></p> </div>\n",
    "         <div style=\"font-size: 20px; width: 800px;\">\n",
    "              <h1>\n",
    "               <left>Eigen Values, Vectors, Decompositions, and Transformations </left>\n",
    "              </h1>\n",
    "              <p><left>============================================================================</left> </p>              \n",
    "             <pre>Course: BIOM/CBE 480A5, Spring 2025\n",
    "Instructor: Dr. Brian Munsky\n",
    "Contact Info: munsky@colostate.edu\n",
    "Authors: Will Raymond, Dr. Brian Munsky\n",
    "</pre>\n",
    "         </div>\n",
    "    </p>\n",
    "\n",
    "</html>\n",
    "\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>Copyright info</summary>\n",
    "\n",
    "```\n",
    "Copyright 2023 Brian Munsky\n",
    "\n",
    "Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n",
    "\n",
    "2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n",
    "\n",
    "3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n",
    "\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "```\n",
    "<details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1 Learning Objectives**\n",
    "\n",
    "In this notebook, we will continue our review of linear algebra, and again focuss on several tools in NumPy.  Upon completing this lesson, students will achieve learning objectives to describe several types of matrices and be able to manipulate them in Python.  Specifically, upon finishing this notebook, you should be able to:\n",
    "\n",
    "* Describe important types of special matrices including ```zero``` matrix, ```identity``` matrix, ```diagonal``` matrix, ```block diagonal``` matrix, ```orthonormal``` matrix, ```unitary``` matrix, ```symmetric``` matrix, and ```positive definite``` matrix.\n",
    "* Understand the meaning of ```image```, ```range```, ```rank```, ```determinant```, and ```nullspace```, and be able to compute these quantities using NumPy.\n",
    "* Describe the meaning of ```eigenvalues``` and ```eigenvectors``` and be able to solve for these in Python.\n",
    "* Be able to use linear algebra operations to ```transform``` data from one set of basis vectors to another.\n",
    "* Describe the meaning and value of ```singular value decomposition```, and be able to perform ```SVD``` on arrays of data in NumPy.\n",
    "* Describe the process of ```Principle Component Analysis``` and be able to perform ```PCA``` on numerical data.\n",
    "\n",
    "We will start again by loading the ```numpy``` modules and some plotting codes in ```matplotlib```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, lets import numpy and matplot lib and set some options for plotting.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting style - here we set some options for plotting later in this notebook.\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.lines import Line2D\n",
    "colors = ['#ef476f', '#06d6a0','#7400b8','#073b4c', '#118ab2',]\n",
    "#colors = ['#fa8174', '#b3de69', '#bc82bd','#ccebc4','#ffed6f','#81b1d2']\n",
    "font = {'family' : 'monospace',\n",
    "    'weight' : 'bold',\n",
    "    'size'   : 12}\n",
    "plt.rcParams.update({'font.size': 12, 'font.weight':'bold', 'font.family':'monospace'  }   )\n",
    "plt.rcParams.update({'axes.prop_cycle':cycler(color=colors)})\n",
    "plt.rcParams.update({'axes.prop_cycle':cycler(color=colors)})\n",
    "plt.rcParams.update({'axes.prop_cycle':cycler(color=colors)})\n",
    "plt.rcParams.update({'xtick.major.width'   : 2.8 })\n",
    "plt.rcParams.update({'xtick.labelsize'   : 12 })\n",
    "plt.rcParams.update({'ytick.major.width'   : 2.8 })\n",
    "plt.rcParams.update({'ytick.labelsize'   : 12})\n",
    "plt.rcParams.update({'axes.titleweight'   : 'bold'})\n",
    "plt.rcParams.update({'axes.titlesize'   : 10})\n",
    "plt.rcParams.update({'axes.labelweight'   : 'bold'})\n",
    "plt.rcParams.update({'axes.labelsize'   : 12})\n",
    "plt.rcParams.update({'axes.linewidth':2.8})\n",
    "plt.rcParams.update({'axes.labelpad':8})\n",
    "plt.rcParams.update({'axes.titlepad':10})  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Different types of special matrices.**\n",
    "\n",
    "During the course, we may need to use a few special types of matrices.  The first five are defined by their shapes:\n",
    "\n",
    "```Zero Matrix```\n",
    "- **Definition**: A matrix in which all elements are zero.\n",
    "- **Notation**: Often denoted as $0$.\n",
    "- **Example**:\n",
    "  \\begin{bmatrix}\n",
    "  0 & 0 \\\\\n",
    "  0 & 0\n",
    "  \\end{bmatrix}\n",
    "\n",
    "\n",
    "```Identity Matrix```\n",
    "- **Definition**: A square matrix with ones on the diagonal and zeros elsewhere.\n",
    "- **Notation**: Often denoted as $I$.\n",
    "- **Example**:\n",
    "  \\begin{bmatrix}\n",
    "  1 & 0 \\\\\n",
    "  0 & 1\n",
    "  \\end{bmatrix}\n",
    "\n",
    "```Diagonal Matrix```\n",
    "- **Definition**: A matrix in which the entries outside the main diagonal are all zero.\n",
    "- **Example**:\n",
    "  \\begin{bmatrix}\n",
    "  1 & 0 & 0 \\\\\n",
    "  0 & 2 & 0 \\\\\n",
    "  0 & 0 & 3\n",
    "  \\end{bmatrix}\n",
    "\n",
    "```Block Diagonal Matrix```\n",
    "- **Definition**: A matrix composed of smaller square matrices (blocks) along its diagonal, with zeros elsewhere.\n",
    "- **Example**:\n",
    "  \\begin{bmatrix}\n",
    "  A & 0 \\\\\n",
    "  0 & B\n",
    "  \\end{bmatrix}\n",
    "  where $A$ and $B$ are square matrices.\n",
    "\n",
    "```Symmetric Matrix```\n",
    "- **Definition**: A square matrix that is equal to its transpose.\n",
    "- **Property**: (A = A^T).\n",
    "- **Example**:\n",
    "  \\begin{bmatrix} \n",
    "1 & 2 & 3\\\\\n",
    "2 & 4 & 5\\\\\n",
    "3 & 5 & 6  \n",
    "\\end{bmatrix} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two additional important matrices (```Orthonormal``` and ```Unitary```) are defined by the orrientation and magnitude of their collumns.\n",
    "\n",
    "```Orthonormal Matrix```\n",
    "- **Definition**: A square matrix whose columns (and rows) are orthonormal vectors (i.e., orthogonal unit vectors).\n",
    "- **Property**: \n",
    "  $\\mathbf{Q}^T \\mathbf{Q} = \\mathbf{Q} \\mathbf{Q}^T = \\mathbf{I}$\n",
    "- **Example**:\n",
    "  \\begin{bmatrix} \n",
    "1 & 0 & 0\\\\\n",
    "0 & \\sqrt{1/2}& \\sqrt{1/2}\\\\\n",
    "0 & \\sqrt{1/2}& -\\sqrt{1/2}  \n",
    "\\end{bmatrix} \n",
    "\n",
    "```Unitary Matrix```\n",
    "- **Definition**: A complex square matrix whose conjugate transpose is also its inverse.\n",
    "- **Property**: \n",
    "  $\\mathbf{U}^* \\mathbf{U} = \\mathbf{U} \\mathbf{U}^* = \\mathbf{I}$\n",
    "- **Example**:\n",
    " $$\\frac{1}{\\sqrt{3}}\\begin{bmatrix} 1 & 1 & 1 \\\\ 1 & e^{2\\pi i / 3} & e^{-2\\pi i / 3} \\\\ 1 & e^{-2\\pi i / 3} & e^{2\\pi i / 3} \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final special type of matrix we will use often is a ```Positive Definite Matrix```.\n",
    "\n",
    "```Positive Definite Matrix```\n",
    "- **Definition**: A symmetric square matrix for which all eigenvalues are positive.\n",
    "- **Properties**: \n",
    "  1) For any non-zero vector $\\mathbf{x}$, the quantity $\\mathbf{x}^T \\mathbf{A x} > 0$.\n",
    "  2) A positive definite matrix can be decomposed into the product of a lower triangular matrix and its transpose, ($\\mathbf{A} = \\mathbf{LL}^T$), where ($\\mathbf{L}$) is a lower triangular matrix with *positive* diagonal entries.\n",
    "- **Example**:\n",
    " $$ A = \\begin{bmatrix} 2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Important properties of matrices.**\n",
    "\n",
    "To introduce a few important properties of square matrices, let's first define a simple 4x4 matrix using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a 4 x 4 matrix in numpy\n",
    "A = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1. Image (or Range)**\n",
    "The **image** (or **range**) of a matrix $\\mathbf{A}$ is the set of all possible output vectors $\\mathbf{b}$ that can be obtained by multiplying $\\mathbf{A}$ with some input vector $\\mathbf{x}$. Mathematically, it is defined as:\n",
    "$$\n",
    "\\text{Im}(\\mathbf{A}) = \\{ \\mathbf{b} \\in \\mathbb{R}^m \\mid \\mathbf{b} = \\mathbf{A} \\mathbf{x} \\text{ for some } \\mathbf{x} \\in \\mathbb{R}^n \\}\n",
    "$$\n",
    "The image of $\\mathbf{A}$ is a subspace of $\\mathbb{R}^m$.\n",
    "\n",
    "The image of the $\\mathbf{A}$ is the space of vectors that are spanned by the columns of the matrix.  Numpy doesn't have a direct means to get the image, but we can write one ourselves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The image of the matrix A is the set of all possible linear combinations of the columns of A.\n",
    "\n",
    "# Let's use numpy to find a minimal set of vectors to span the image of A\n",
    "rank_A = np.linalg.matrix_rank(A)\n",
    "print('The rank of A is:', rank_A)\n",
    "\n",
    "# The rank of A is 2, so the image of A is spanned by the first two linearly independent columns of A.\n",
    "# Let's write a simple python function to find these columns.\n",
    "def find_image(A):\n",
    "    # Function that takes a matrix A and returns a matrix whose columns span the image of A.\n",
    "    image_A = A[:,0]\n",
    "    j = 0\n",
    "    rank_Image = 1\n",
    "    while rank_Image < rank_A:\n",
    "        j += 1\n",
    "        if rank_Image < np.linalg.matrix_rank(np.vstack((image_A, A[:,j]))):\n",
    "            image_A = np.vstack((image_A, A[:,j]))\n",
    "            rank_Image += 1\n",
    "    return np.array([image_A.T[:,i]/np.linalg.norm(image_A.T[:,i]) for i in range(rank_Image)]).T\n",
    "\n",
    "print('The image of A is spanned by the columns:\\n', find_image(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2. Null Space**\n",
    "The **null space** (or **kernel**) of a matrix $\\mathbf{A}$ is the set of all vectors $\\mathbf{x}$ that satisfy the equation $\\mathbf{A} \\mathbf{x} = \\mathbf{0}$. Mathematically, it is defined as:\n",
    "$$\\text{Null}(\\mathbf{A}) = \\{ \\mathbf{x} \\in \\mathbb{R}^n \\mid \\mathbf{A} \\mathbf{x} = \\mathbf{0} \\}$$\n",
    "The null space of $\\mathbf{A}$ is a subspace of $\\mathbb{R}^n$.\n",
    "\n",
    "Unfortunately, numpy doesn't have a direct function for null space, so we will wait until we have described eigenvectors to do an example for this one. (Note, scipy.linalg does have such a routine.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **3.3 Determinant**\n",
    "The **determinant** of a square matrix $\\mathbf{A}$ is a scalar value that provides important information about the matrix. It is denoted as $\\det(\\mathbf{A})$ or $|\\mathbf{A}|$. The determinant has several key properties:\n",
    "- It indicates whether the matrix is invertible. A matrix $\\mathbf{A}$ is invertible if and only if $\\det(\\mathbf{A}) \\neq 0$.\n",
    "- It provides information about the volume scaling factor of the linear transformation described by the matrix.\n",
    "- It can be computed using various methods, including cofactor expansion and row reduction.\n",
    "\n",
    "\n",
    "For a $2 \\times 2$ matrix $\\mathbf{A} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}$, the determinant is given by:\n",
    "$$\n",
    "\\det(\\mathbf{A}) = ad - bc\n",
    "$$\n",
    "\n",
    "For a $3 \\times 3$ matrix $\\mathbf{A} = \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{bmatrix}$, the determinant is given by:\n",
    "$$\n",
    "\\det(\\mathbf{A}) = a(ei - fh) - b(di - fg) + c(dh - eg)\n",
    "$$\n",
    "\n",
    "For larger matrices, the determinant can found using the recursive method known as ```cofactor expansion method```.  We will just use numpy routines.\n",
    "\n",
    "These concepts are fundamental in linear algebra and have wide-ranging applications in various fields of science and engineering.\n",
    "\n",
    "We will solve for the determinant of our 4x4 matrix from above, but before we do...**Do you know what the determinant is already?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the determinant of A.\n",
    "\n",
    "det_A = np.linalg.det(A)\n",
    "print('The determinant of A is:', det_A)\n",
    "\n",
    "# The determinant of A is zero, because we already know that the rank of A is less than the number of columns in A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.4 Inverse of Square, Full Rank Matrices**\n",
    "\n",
    "The **inverse** of a matrix $\\mathbf{A}$ is another matrix, denoted as $\\mathbf{A}^{-1}$, such that when $\\mathbf{A}$ is multiplied by $\\mathbf{A}^{-1}$, the result is the identity matrix $\\mathbf{I}$. Mathematically, this is expressed as:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conditions for Invertibility:**\n",
    "1. **Square Matrix**: Only square matrices (matrices with the same number of rows and columns) can have an inverse.\n",
    "2. **Non-Singular Matrix**: A matrix $\\mathbf{A}$ is invertible if and only if it is non-singular, meaning its determinant is non-zero ($\\det(\\mathbf{A}) \\neq 0$).  In other words, the matrix rank must be equal to its height and width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Finding the Inverse:**\n",
    "There are several methods to find the inverse of a matrix, including the adjoint method, Gaussian elimination, and using matrix decomposition techniques. Here, we will describe the adjoint method for a $2 \\times 2$ matrix and provide a general approach for larger matrices.\n",
    "\n",
    "#### **Inverse of a $2 \\times 2$ Matrix:**\n",
    "For a $2 \\times 2$ matrix $\\mathbf{A}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The inverse $\\mathbf{A}^{-1}$ is given by:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}^{-1} = \\frac{1}{\\det(\\mathbf{A})} \\begin{bmatrix}\n",
    "d & -b \\\\\n",
    "-c & a\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where the determinant $\\det(\\mathbf{A}) = ad - bc$.\n",
    "\n",
    "#### **General Method for Larger Matrices:**\n",
    "For an $n \\times n$ matrix $\\mathbf{A}$, the inverse can be found using the adjoint method or Gaussian elimination.\n",
    "\n",
    "1. **Adjoint Method**:\n",
    "   - Compute the matrix of cofactors.\n",
    "   - Transpose the matrix of cofactors to get the adjugate (or adjoint) matrix $\\mathbf{A}^\\text{adj}$.\n",
    "   - Divide the adjugate matrix by the determinant of $\\mathbf{A}$:\n",
    "\n",
    "   $$\n",
    "   \\mathbf{A}^{-1} = \\frac{1}{\\det(\\mathbf{A})} \\mathbf{A}^\\text{adj}\n",
    "   $$\n",
    "\n",
    "2. **Gaussian Elimination**:\n",
    "   - Augment the matrix $\\mathbf{A}$ with the identity matrix $\\mathbf{I}$ to form $[ \\mathbf{A} | \\mathbf{I} ]$.\n",
    "   - Perform row operations to transform $\\mathbf{A}$ into the identity matrix.\n",
    "   - The resulting augmented matrix will be $[ \\mathbf{I} | \\mathbf{A}^{-1} ]$.\n",
    "\n",
    "### **Example:**\n",
    "Consider the matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix}\n",
    "4 & 7 \\\\\n",
    "2 & 6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "1. **Determinant**:\n",
    "   $$\n",
    "   \\det(\\mathbf{A}) = 4 \\cdot 6 - 7 \\cdot 2 = 24 - 14 = 10\n",
    "   $$\n",
    "\n",
    "2. **Inverse**:\n",
    "   $$\n",
    "   \\mathbf{A}^{-1} = \\frac{1}{10} \\begin{bmatrix}\n",
    "   6 & -7 \\\\\n",
    "   -2 & 4\n",
    "   \\end{bmatrix} = \\begin{bmatrix}\n",
    "   0.6 & -0.7 \\\\\n",
    "   -0.2 & 0.4\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "### **Python Example:**\n",
    "Here is how you can compute the inverse of a matrix using NumPy in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the matrix\n",
    "A = np.array([[4, 7], [2, 6]])\n",
    "\n",
    "# Compute the inverse\n",
    "A_inv = np.linalg.inv(A)\n",
    "\n",
    "print(\"Inverse of A:\\n\", A_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Eigenvalues and Eigenvectors of Square Matrices**\n",
    "\n",
    "**Eigenvalues** and **eigenvectors** are fundamental concepts in linear algebra with numerous applications in various fields such as physics, engineering, computer science, and data analysis.\n",
    "\n",
    "## **4.1. Definition of Eigenvalue-Eigenvector pairs:**\n",
    "- **Eigenvector**: An eigenvector of a square matrix **A** is a non-zero vector **v** such that when **A** is multiplied by **v**, the result is a scalar multiple of **v**. Mathematically, this is expressed as:\n",
    "  $$\n",
    "  \\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}\n",
    "  $$\n",
    "  where **v** is the eigenvector and $\\lambda$ is the corresponding eigenvalue.\n",
    "\n",
    "- **Eigenvalue**: An eigenvalue $\\lambda$ is a scalar that satisfies the above equation for a given eigenvector **v**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2. Finding Eigenvalues and Eigenvectors:**\n",
    "1. **Characteristic Equation**: To find the eigenvalues of a matrix **A**, solve the characteristic equation:\n",
    "   $$\n",
    "   \\det(\\mathbf{A} - \\lambda \\mathbf{I}) = 0\n",
    "   $$\n",
    "   where $\\det()$ is an operation that returns the determinant, and **I** is the identity matrix of the same dimension as **A**.\n",
    "\n",
    "2. **Eigenvectors**: Once the eigenvalues are found, the corresponding eigenvectors can be determined by solving the equation $ (\\mathbf{A} - \\lambda \\mathbf{I})\\mathbf{v} = 0 $ for each eigenvalue $\\lambda$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PI-mbvbl3U8-"
   },
   "source": [
    "## **4.3. Computing Eigenvalues and Eigenvectors in Numpy**\n",
    "Numpy provides some simple codes to calculate the \n",
    "\n",
    "| Method    | Description  | Examples  \n",
    "| ----------- | ----------- | ------------ |\n",
    "| np.linalg.eig | returns the eigen vectors and values of a matrix| |  \n",
    "| np.linalg.eigvals | returns the eigenvalues of a matrix |\n",
    "\n",
    "For a more in depth explanation  check out the following youtube video: [\"Eigenvectors and eigenvalues | Chapter 14, Essence of linear algebra\"](https://www.youtube.com/watch?v=PFDu9oVAE-g)\n",
    "\n",
    "## **4.4. Example Eigenvalue/Vector Calculations**\n",
    "\n",
    "Let's do some eigenvalue examples, and as we do so let's learn some important facts about the eigenvalues of common matrices.\n",
    "\n",
    "### **Eigenvalues of diagonal matrices.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the eigenvalues and eigenvectors of diagonal matrices.\n",
    "# Define a diagonal matrix\n",
    "D = np.diag([1, 2, 3, 4])\n",
    "\n",
    "# Compute the eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(D)\n",
    "\n",
    "print(\"Eigenvalues of D:\\n\", eigenvalues)\n",
    "print(\"Eigenvectors of D:\\n\", eigenvectors)\n",
    "\n",
    "# Note, for diagonal matrices, the eigenvalues are the just thediagonal entries of the \n",
    "# matrix, and the eigenvectors are the standard basis vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Eigenvalues for lower or upper triangular matrices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what we get as the eigenvalues and eigenvectors of a lower triangular matrix.\n",
    "# Define a lower triangular matrix\n",
    "L = np.array([[1, 0, 0], [2, 3, 0], [4, 5, 6]])\n",
    "print('Our lower triangular matrix L is:\\n', L)\n",
    "\n",
    "# Compute the eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(L)\n",
    "\n",
    "print(\"Eigenvalues of L:\\n\", eigenvalues)\n",
    "print(\"Eigenvectors of L:\\n\", eigenvectors)\n",
    "\n",
    "# The eigenvalues of a lower triangular matrix are the diagonal entries of the matrix. \n",
    "# The matrix formed by the eigenvectors can also be arranged to be lower triangular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Eigenvalues for symmetric matrix**\n",
    "\n",
    "For a ```symmetric matrix```, all eigenvalues are real, and the eigenvectors form an ```orthonormal matrix```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cp5oSUx8BPmI",
    "outputId": "6ce91724-630c-49c1-eb89-e38c49cdfb21"
   },
   "outputs": [],
   "source": [
    "# Create a random symmetrix matrix\n",
    "random_matrix = np.random.randn(4,4)\n",
    "random_matrix = random_matrix+random_matrix.T\n",
    "\n",
    "# compute eigenvalue/eigenvector pairs\n",
    "eigenvals, eigenvecs = np.linalg.eig(random_matrix)\n",
    "\n",
    "print('Random symmetric matrix:')\n",
    "print(random_matrix)\n",
    "\n",
    "print('\\nEigenvectors:')\n",
    "print(eigenvecs)\n",
    "print('\\nEigenvalues:')\n",
    "print(eigenvals)\n",
    "\n",
    "# Note that for symmetric matrices, the eigenvalues and eigenvectors are always real. \n",
    "\n",
    "# Let's check that the eigenvectors are orthonormal.\n",
    "print('Check that eigenvectors are orthonormal:')\n",
    "print(f'eigenvecs.T @ eigenvecs:\\n{eigenvecs.T @ eigenvecs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using Eigenvalues and Eigenvectors to Determine the Image and Nullspace of a Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the 4x4 matrix A from above.\n",
    "A = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]])\n",
    "print('Matrix A:\\n', A)\n",
    "\n",
    "# Compute the eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "print(\"Eigenvalues of A:\\n\", eigenvalues)\n",
    "print(f'The rank of A is: {np.linalg.matrix_rank(A)}')\n",
    "print(f'The number of non-zero eigenvalues of A is: {np.count_nonzero(abs(eigenvalues)>1e-10)}')\n",
    "# The rank of A is 2, and the number of non-zero eigenvalues of A is 2.\n",
    "\n",
    "# The range of A is spanned by the eigenvectors corresponding to the non-zero eigenvalues of A.\n",
    "print(f'The range of A is spanned by:\\n {eigenvectors[:,abs(eigenvalues)>1e-10]}')\n",
    "\n",
    "# The null space of A is spanned by the eigenvectors corresponding to the zero eigenvalues of A.\n",
    "print(f'The null space of A is spanned by:\\n {eigenvectors[:,abs(eigenvalues)<1e-10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check that the eigenvectors are orthogonal to each other.\n",
    "print('The dot product of the eigenvectors is:\\n', np.dot(eigenvectors.T, eigenvectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check that the nullspace eigenvectors are orthogonal to the image that we\n",
    "# computed earlier.\n",
    "image_A = find_image(A)\n",
    "nullspace_A = eigenvectors[:,abs(eigenvalues)<1e-10]\n",
    "print('The dot product of the image and nullspace eigenvectors is:\\n', np.dot(image_A.T, nullspace_A))\n",
    "print('All the dot products are zero, so the image and nullspace eigenvectors are orthogonal to each other.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Common Eigenvalue/Eigenvector Applications:**\n",
    "- **Stability Analysis**: In systems of differential equations, eigenvalues can determine the stability of equilibrium points.\n",
    "- **Principal Component Analysis (PCA)**: In data analysis, eigenvectors of the covariance matrix represent the principal components.\n",
    "- **Quantum Mechanics**: Eigenvalues and eigenvectors are used to solve the Schrödinger equation.\n",
    "\n",
    "Eigenvalues and eigenvectors provide deep insights into the properties of linear transformations and are essential tools in both theoretical and applied mathematics. Later in the course, we will use them both for data analysis as well as for solving large sets of linear ODEs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3v-7TCNkujR"
   },
   "source": [
    "# **3. Linear Transformations**\n",
    "\n",
    "A ```linear transformation``` is a simple linear operation that transforms a vector from one  basis vector space (i.e., where all basis vectors are orthogonal to one another) to another, potentially more convenient cordinate definition.  \n",
    "\n",
    "To achieve such a transformation, we can multiply any vector in the original coordinate system by a orthogonal matrix (a matrix whose collumns are orthogonal to one another).\n",
    "\n",
    "For example, suppose that we examine a point in 2D denoted by [x,y] in the regular cartesian coordinate system. We can rotate this coordinate systme about the origin [0,0] by an angle $\\theta$ (defined as positive in the counter clockwise direction), using the linear transformation:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "x'\\\\\n",
    "y'\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "\\cos(\\theta) & -\\sin(\\theta)\\\\\n",
    "\\sin(\\theta) & \\cos(\\theta) \\\\\n",
    "\\end{bmatrix} * \\begin{bmatrix}\n",
    "x\\\\\n",
    "y\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "You chould confirm on your own that this transformation matrix is indeed a orthonormal matrix (i.e., that all columns are orthogonal and have magnitude of one) by showing that if $\\mathbf{v}_i$ and $\\mathbf{v}_j$ are both columns of the transformation matrix, then the dot product $\\mathbf{v}_i\\cdot \\mathbf{v}_j$ is one if $i=j$ and zero if $i\\ne j$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "TdSpbrTItep6",
    "outputId": "d6a1b217-7d28-4a10-a6ab-01ac9e3142da"
   },
   "outputs": [],
   "source": [
    "# Generate a large number of random 2D points\n",
    "xy = np.random.randint(0,100,size=(2,100)) # a random constellation of points at integer values\n",
    "\n",
    "# Choose a central point for rotation\n",
    "center = np.array([[30],[15]]) # axis of rotation\n",
    "\n",
    "# Perform rotation at three different angles\n",
    "thetas = [.02, .1, .2]\n",
    "\n",
    "# Define the transformation matrix for our 2D rotation\n",
    "transformation_mat = lambda theta: np.array([[np.cos(theta), -np.sin(theta)],\n",
    "                                            [np.sin(theta),  np.cos(theta)]])\n",
    "\n",
    "# We are going to translate the points to the new center point, \n",
    "# then apply the transformation matrix application of the rotation:  \n",
    "# A^-1 (2x2) @ (xy (100x2)- rotation point (2x1)).T (2 x 100)\n",
    "new_xy1 = transformation_mat(thetas[0]) @ (xy - center) + center\n",
    "new_xy2 = transformation_mat(thetas[1]) @ (xy - center) + center\n",
    "new_xy3 = transformation_mat(thetas[2]) @ (xy - center) + center\n",
    "\n",
    "# Now to plot our results:\n",
    "plt.plot(center[0], center[1], 'x')\n",
    "plt.scatter(*xy)\n",
    "plt.scatter(*new_xy1)\n",
    "plt.scatter(*new_xy2)\n",
    "plt.scatter(*new_xy3)\n",
    "plt.legend(['center', 'original','.02 rad', '.1 rad', '.2 rad'], bbox_to_anchor=(1.05, 1.05))\n",
    "plt.title('rotating a 100 2D points w/ rotation matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGtqR_dSxefY"
   },
   "source": [
    "### **Combining multiple sequential transformation steps**\n",
    "Notice in the previous example, that we had to take three steps: \n",
    "*(1) we translated the data to place the center of rotation at (0,0), \n",
    "*(2) we rotated all the vectors about the origin (0,0), and then \n",
    "*(3) we translated back to the original origin.\n",
    " \n",
    " With clever application of linear algebra, we can do all three of these steps in one go using an Affine Transformation Matrix which can be constructed as the product of multiple transformation matrixes. \n",
    "\n",
    "First, we define the initial translation:\n",
    "$\n",
    "M1 = \\begin{bmatrix}\n",
    "1 & 0 & -\\Delta x\\\\\n",
    "0 & 1 & -\\Delta y\\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Here the first row describes how $x$ changes, the second row describes how $y$ changes, and the third row shows how a hidden third variable changes. You could think of this as $z$ or a bias magnitude - either way, it will remain constant at one.  Similarly, the first column is how the new coordinate **depends** on $x$, the second is how the new coordinate depends on $y$ and the third how it depends on the bias.  Because this just a simple shift, $x$ and $y$ take on thier original values plus a multiple of the bias term.\n",
    "\n",
    "Next, we define the rotation about the origin.\n",
    "$M2 = \\begin{bmatrix}\n",
    "\\cos(\\theta) & -\\sin(\\theta) & 0\\\\\n",
    "\\sin(\\theta) & \\cos(\\theta) & 0\\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "In the second transformation, the $x$ and $y$ coordinates are rotated independent of the $z$/bias term.\n",
    "\n",
    "Third, we define the final translation.\n",
    "$\n",
    "M3 = \\begin{bmatrix}\n",
    "1 & 0 & \\Delta x\\\\\n",
    "0 & 1 & \\Delta y\\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "This translation term is the same as the first one, but its bias-shift now has the opposite sign.\n",
    "\n",
    " We have to keep that order here too when we apply these transformation matrices, so the entire transformation is $MT = M3\\cdot M2\\cdot M1$ (note this reads from right to left).\n",
    "\n",
    "Applying the transformation to a set of points:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "x'\\\\\n",
    "y'\\\\\n",
    "1\n",
    "\\end{bmatrix} =\n",
    "M3\\cdot M2 \\cdot M1 \\cdot \\begin{bmatrix}\n",
    "x\\\\\n",
    "y\\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "You can try other affine transformation matrices as well -- but for now we are just demonstrating the common linear algebra transformation in ```numpy```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "lNz-gIN9wyLa",
    "outputId": "4616ed26-7911-49be-ed81-d8f80c7bded8"
   },
   "outputs": [],
   "source": [
    "# Let's try it:  First, we add in a column of 1s to our x,y constellation so now its 3x100:\n",
    "print(len(xy[0,:]))\n",
    "\n",
    "# Create a matrix with the x-values in the first row, y-values in the second row, \n",
    "# and 1s in the third row. You could think of the third row as either the z-values\n",
    "# in 3D space, or as a row of 1s that will allow us to add a bias term to our\n",
    "# linear transformation.\n",
    "xy_appended = np.vstack([xy, np.ones([1,len(xy[0,:])])])\n",
    "\n",
    "# The transformation matrix for the initial bias shift.\n",
    "M1 = lambda rx, ry, theta: np.array([[1, 0, -rx],\n",
    "                                    [0,  1, -ry],\n",
    "                                    [0,  0,  1]])\n",
    "\n",
    "# The transformation matrix for the rotation.\n",
    "M2 = lambda rx, ry, theta: np.array([[np.cos(theta), -np.sin(theta), 0],\n",
    "                                    [np.sin(theta), np.cos(theta), 0],\n",
    "                                    [0,              0,             1]])\n",
    "\n",
    "# The transformation matrix for the final bias shift.\n",
    "M3 = lambda rx, ry, theta: np.array([[1, 0, rx],\n",
    "                                    [0,  1, ry],\n",
    "                                    [0,  0,  1]])\n",
    "\n",
    "# Create a lambda function that will apply all three transformations.\n",
    "MT = lambda rx, ry, theta: M3(rx,ry,theta)@M2(rx,ry,theta)@M1(rx,ry,theta)\n",
    "\n",
    "#Set the center point about which to rotate: \n",
    "rx = center[0,0]\n",
    "ry = center[1,0]\n",
    "\n",
    "#Set the angle to rotate counter-clockwise (radians)\n",
    "theta = 0.2\n",
    "\n",
    "# Apply the transformation matrix to the appended matrix of points.\n",
    "xy_translated = MT(rx,ry,theta) @ xy_appended\n",
    "\n",
    "# Now to plot our results:\n",
    "plt.plot(rx, ry, 'x')\n",
    "plt.scatter(*xy_appended[:2]) # only plot along the first two dimensions\n",
    "plt.scatter(*xy_translated[:2])\n",
    "plt.scatter(new_xy3[0,:],new_xy3[1,:],marker='.')\n",
    "\n",
    "plt.legend(['center', 'original',f'{theta} rad (new approach)',f'{theta} rad (old approach)'], bbox_to_anchor=(1.05, 1.05))\n",
    "plt.title('rotating a 100 2D points w/ affine rotation matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tocGXL9wkype"
   },
   "source": [
    "# **3. Singular Value Decomposition and Principle Component Analysis**\n",
    "\n",
    "```Singular value decomposition``` is an efficient linear algebra method that generalizes the eigenvalue-eigenvector decomposition from above to allow similar decompositons to be performed on rectangular matrices (e.g., like large data sets). SVD decomposes a $m\\times n$ matrix $\\mathbf{X}$ as \n",
    "$\\mathbf{X} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T,$\n",
    "where:\n",
    "\n",
    "* $\\mathbf{U}$ is an ```orthonormal``` matrix containing the ```left singular vectors``` of $\\mathbf{X}$.  The columns of $\\mathbf{U}$ are the eigenvectors of the symmetric matrix $\\mathbf{X}^T\\mathbf{X}$.\n",
    "* $\\boldsymbol{\\Sigma}$ is a diagonal matrix with ```singular values``` on the diagonal. The non-zero entries are the square roots of the non-zero eigenvalues of $\\mathbf{X}^T\\mathbf{X}$￼ (which overlap with those for $\\mathbf{X}^T\\mathbf{X}$).\n",
    "* $\\mathbf{V}^T$ is the transpose of an orthogonal matrix containing the ```right singular vectors``` of $\\mathbf{X}$. The columns of $\\mathbf{V}$ are the eigenvectors of the symmetric matrix $\\mathbf{X}\\mathbf{X}^T$.\n",
    "\n",
    "### **Applications of SVD:**\n",
    "1. **Dimensionality Reduction**: SVD is used in Principal Component Analysis (PCA) to reduce the dimensionality of data while preserving as much variance as possible.\n",
    "2. **Image Compression**: SVD can be used to approximate images with fewer singular values, reducing storage requirements while maintaining image quality.\n",
    "3. **Signal Processing**: SVD is used to filter noise from signals and to identify underlying patterns.\n",
    "4. **Least Squares Problems**: SVD provides a stable method for solving linear least squares minimization problems (i.e., solving for the best fit of overdetermined sets of linear equations).\n",
    "\n",
    "Later in the course, we will use SVD for all of these tasks.\n",
    "\n",
    "### **Example of SVD:**\n",
    "Consider the matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & 2 \\\\\n",
    "0 & 0 & 3 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 4 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The SVD of $\\mathbf{A}$ is:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T\n",
    "$$\n",
    "and can be found in python using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example showing how to compute the SVD of a matrix in numpy.\n",
    "# Define the matrix\n",
    "A = np.array([\n",
    "    [1, 0, 0, 0, 2],\n",
    "    [0, 0, 3, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 4, 0, 0, 0]\n",
    "])\n",
    "\n",
    "# Compute the SVD\n",
    "U, Sigma, Vt = np.linalg.svd(A, full_matrices=True)\n",
    "print('SVD Results using the full_matrices=True option:')\n",
    "print(\"U:\\n\", U)\n",
    "print(\"Sigma:\\n\", Sigma)\n",
    "print(\"Vt:\\n\", Vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the SVD using the reduced form\n",
    "U, Sigma, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "print('\\nSVD Results using the full_matrices=False option:')\n",
    "print(\"U:\\n\", U)\n",
    "print(\"Sigma:\\n\", Sigma)\n",
    "print(\"Vt:\\n\", Vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check that the SVD is correct by reconstructing the matrix A.\n",
    "A_reconstructed = U @ np.diag(Sigma) @ Vt\n",
    "print('\\nReconstructed matrix A:\\n', A_reconstructed)\n",
    "\n",
    "# Check and see that this matches the original matrix A!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Principle Component Analysis**\n",
    "```Principal component analysis ```(PCA) is a specific linear transformation that chooses a set of ordered basis vectors (i.e., a new coordinate system) where each orthoganal axis is a \"Principal Component,\" usually arranged in an order such that each new vector captures the next greatest percentage of variation in the given dataset.  In other words, the first principal component is the direction that has the most variation, the second has the next most, etc.  PCA is efficiently solved using the technique of SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Steps to Perform PCA Using SVD**\n",
    "\n",
    "Here are the steps to perform PCA using SVD directly on the data matrix:\n",
    "\n",
    "1. **Standardize the Data**:\n",
    "   - Center the data by subtracting the mean of each feature.\n",
    "   - Optionally, scale the data to have unit variance.  This is important when different measurements in the data have very different scales\n",
    "\n",
    "2. **Perform Singular Value Decomposition (SVD)**:\n",
    "   - Decompose the centered data matrix using SVD to obtain the matrices $\\mathbf{U}$, $\\mathbf{\\Sigma}$, and $\\mathbf{V}^T$.\n",
    "\n",
    "3. **Select Principal Components**:\n",
    "   - Choose the top $k$ singular values and their corresponding singular vectors to form the principal components.\n",
    "\n",
    "4. **Transform the Data**:\n",
    "   - Project the original data onto the selected principal components to obtain the reduced-dimensional representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example using SVD and PCA to Transform Data**\n",
    "We can use SVD to obtain the largest orthonormal eigenbasis for any m x n matrix, which is precisely the operation needed to perform PCA! We will show two implementations below, but first we need some data!\n",
    "\n",
    "A 2D multivariate gaussian is one of the best ways to visualize this particular transformation (without dimensionality reduction)! So let's generate one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by defining some functions to use for our PCA implementation.\n",
    "# Step 1: Standardize the data\n",
    "def normalize_data(X):\n",
    "    mean = np.mean(X, axis=0)\n",
    "    X_centered =(X - mean)\n",
    "    return X_centered, mean\n",
    "\n",
    "# Step 2: Perform SVD\n",
    "def perform_svd(X_centered):\n",
    "    U, Sigma, Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
    "    return U, Sigma, Vt\n",
    "\n",
    "# Step 3: Select principal components\n",
    "def select_principal_components(Vt, k):\n",
    "    V_k = Vt.T[:, :k]\n",
    "    return V_k\n",
    "\n",
    "# Step 4: Transform the data\n",
    "def transform_data(X_centered, V_k):\n",
    "    X_reduced = np.dot(X_centered, V_k)\n",
    "    return X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "tqXfuo5p63A-",
    "outputId": "d998c5fc-d5a4-4e8e-cf68-20f09ad7f454"
   },
   "outputs": [],
   "source": [
    "# Specify some parameters to define our distribution:\n",
    "mu = np.array([3.6,2.7])\n",
    "sig = np.array([[2,-1],[-1,2]])\n",
    "\n",
    "# Generate random data from multivariate gaussian:\n",
    "random_gaussian = np.random.multivariate_normal(mu,sig, size=(1000))\n",
    "\n",
    "# Plot the data:\n",
    "plt.scatter(random_gaussian[:,0], random_gaussian[:,1])\n",
    "plt.xlim([-10,10])\n",
    "plt.ylim([-10,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_5xOoEGiO5_g",
    "outputId": "aad93346-c7a7-43b3-d4db-9b3367b45c96"
   },
   "outputs": [],
   "source": [
    "# PCA using  SVD\n",
    "\n",
    "# PCA requires CENTERED data (zero mean), so let's subtract the mean:\n",
    "centered_gaussian, meandata = normalize_data(random_gaussian)\n",
    "\n",
    "# Let's plot the recenterred data\n",
    "plt.scatter(centered_gaussian[:,0], centered_gaussian[:,1])\n",
    "plt.xlim([-10,10])\n",
    "plt.ylim([-10,10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the SVD:\n",
    "U, Sigma, Vt = perform_svd(centered_gaussian)\n",
    "\n",
    "# This results in:\n",
    "# U = unitary matrix (possibly complex) for the range (columns space) of the matrix. \n",
    "# Sigma = singular values (non-negative real values)\n",
    "# Vt = unitary matrix (possibly complex) for the \n",
    "\n",
    "print(f'The shape of U is: {U.shape}')\n",
    "print(f'The shape of Sigma is: {Sigma.shape}')\n",
    "print(f'The shape of Vt is: {Vt.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "AjWhg_BcjEJy",
    "outputId": "a08caf8f-2c6e-4fd6-9815-3ada92ebe589"
   },
   "outputs": [],
   "source": [
    "# Now let plot the transformed data on the new basis:\n",
    "X_new = centered_gaussian @ Vt # transform our original data after being centered\n",
    "\n",
    "# Make a set of two plots side by side\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(random_gaussian[:,0], random_gaussian[:,1])\n",
    "plt.xlim([-10,10])\n",
    "plt.ylim([-10,10])\n",
    "plt.quiver(*mu, *Vt[:,0], color=['k'], scale=np.sqrt(Sigma[1]))\n",
    "plt.quiver(*mu, *Vt[:,1], color=['c'], scale=np.sqrt(Sigma[0]))\n",
    "plt.legend(['Original Data','PC1','PC2'])\n",
    "plt.title('Data in Original Coordinates')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X_new[:,0], X_new[:,1])\n",
    "plt.xlim([-10,10])\n",
    "plt.ylim([-10,10])\n",
    "#plot the eigen vectors by their eigen values!\n",
    "plt.quiver(*np.mean(X_new,axis=0), *(np.eye(2))[:,0], color=['k'], scale=np.sqrt(Sigma[1]))\n",
    "plt.quiver(*np.mean(X_new,axis=0), *(np.eye(2))[:,1], color=['c'], scale=np.sqrt(Sigma[0]))\n",
    "plt.legend(['Transformed Data','PC1','PC2'])\n",
    "plt.title('Data in New Coordinates')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3ZUpBRlnPZ5"
   },
   "source": [
    "Note how the original gaussian has been placed on its 2 new axes by rotating the original data. The new data looks like an ellipse with its longest direction in PC1 and its smaller direction in PC2.Both the original data and its transformation were in two dimensions, so the actual shape of the data cloud does not apper to have changed.\n",
    "\n",
    "## **Using PCA to reduce data dimension**\n",
    "\n",
    "The power of PCA is in how it hands much larger dimensional data.  The approach will still rotate the data so that the largest variation is about PC1, then PC2, etc., but now that rotation can be about a much larger number of axes.  \n",
    "\n",
    "Let's apply PCA to a larger dimensional data set, but this time instead of keeping the whole result, we will only keep the first few dimensions of the PCA. \n",
    "\n",
    "Our reconstruction will be imperfect, but we can calculate the exact amount of variance captured by including each axis so we can stop once we have collected enough of the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by creating a more complesdata set that contains a mixture of two different \n",
    "# types of multivariate random variables each with slightly different means and variances.:\n",
    "\n",
    "# We start by creating a random mean and covariance matrix for the two types of random variables.\n",
    "nVariables = 20\n",
    "\n",
    "# Let's generate a pair of random means for two types of data\n",
    "mn1 = np.random.rand(nVariables)\n",
    "mn2 = np.random.rand(nVariables)\n",
    "\n",
    "# Let's also generate a pair of random covariance matrices.\n",
    "sig1 = np.cov(np.random.rand(nVariables,6))\n",
    "sig2 = np.cov(np.random.rand(nVariables,6))\n",
    "\n",
    "# Next, we generate many samples from each distribution:\n",
    "nSamples = 1000\n",
    "samples1 = np.random.multivariate_normal(mn1,sig1,size=nSamples)\n",
    "samples2 = np.random.multivariate_normal(mn2,sig2,size=nSamples)\n",
    "\n",
    "# Now to combine the two data sets into a single normalized data set:\n",
    "X = np.vstack([samples1,samples2])\n",
    "std = np.sqrt(np.var(X,axis=0))\n",
    "X = (X-np.mean(X,axis=0))/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's plot these two sample sets on a couple of their axes:\n",
    "ax0 = 0\n",
    "ax1 = 1\n",
    "plt.scatter(X[:1000,ax0], X[:1000,ax1])\n",
    "plt.scatter(X[1000:,ax0], X[1000:,ax1])\n",
    "plt.legend(['Set 1','Set 2'])\n",
    "plt.xlabel(f'Axis {ax0}')\n",
    "plt.ylabel(f'Axis {ax1}')\n",
    "plt.title('Two Sets of Random Variables (Our original data)')\n",
    "\n",
    "# Note that the two data types are similar but not exactly the same.\n",
    "# Try different combinations to see if you can separate out the two colors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOBWfEeoug2p"
   },
   "source": [
    "Now we can \"Select\" components by keeping only the first few PCA coordinates. \n",
    "\n",
    "Using only the first two components (first 2 indexes) of each matrix will let us construct some $\\hat{X}$ that is a lossy representation of our original data, $X$. Play with the number of components included, and see how the error of $\\hat{X}$ vs $X$ changes in relation to number of components used!\n",
    "\n",
    "We can also calculate the explained variance by squaring the sum of the singular values kept divided by the total singular values squared!\n",
    "\n",
    "$ Explained\\: Variance = S[:N_{components}]^2 / S^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1L4ZemAt9w3",
    "outputId": "cba68f3c-ed91-4204-8e75-aac0366b235b"
   },
   "outputs": [],
   "source": [
    "# Compute the SVD transformation of the zero-mean data:\n",
    "U, S, V = np.linalg.svd(X) #single value decomp with numpy\n",
    "\n",
    "components = [0,1,2,3,4]\n",
    "X_hat = U[:,components] @ (np.diag(S[components]) @ V[components,:]) # apply the transformation to the data\n",
    "explained_variance = np.sum(S[components]**2/np.sum(S**2))\n",
    "X_reduced = (X @ V[components,:].T)\n",
    "\n",
    "# Let's plot the original and transformed data on two different axes:\n",
    "ax0 = 0\n",
    "ax1 = 1\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X[:1000,ax0], X[:1000,ax1])\n",
    "plt.scatter(X[1000:,ax0], X[1000:,ax1])\n",
    "plt.legend(['Set 1','Set 2'])\n",
    "plt.xlabel(f'Axis {ax0}')\n",
    "plt.ylabel(f'Axis {ax1}')\n",
    "plt.title('Original Data')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X_hat[:1000,ax0], X_hat[:1000,ax1])\n",
    "plt.scatter(X_hat[1000:,ax0], X_hat[1000:,ax1])\n",
    "plt.legend(['Set 1','Set 2'])\n",
    "plt.xlabel(f'Axis {ax0}')\n",
    "plt.ylabel(f'Axis {ax1}')\n",
    "plt.title('Reduced Data')\n",
    "\n",
    "print('MSE transformed data to original data:')\n",
    "print(np.sum(np.subtract(X_hat, X)**2))\n",
    "print('')\n",
    "print(f'Total explained variance of components: {components}')\n",
    "print('{} %'.format(explained_variance*100))\n",
    "print('')\n",
    "print('reduced dimension:')\n",
    "print(X_reduced.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how much of the variance can be retained as a function of the number of principle components are retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the variance explained by each component.\n",
    "# Here we will make two plots.  The first will show the variance explained by each component,\n",
    "# and the second will show the cumulative variance explained by the components.\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(np.arange(len(S)), S**2/np.sum(S**2))\n",
    "plt.xlabel('Component')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.title('Variance Explained by Each Component')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(np.cumsum(S**2/np.sum(S**2)))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Variance Explained')\n",
    "plt.title('Cumulative Variance Explained by Components')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwTdHE102ek4"
   },
   "source": [
    "So by keeping just a few principal components, we are able to reconstruct most of the variations in our data! Another way of saying this is that we can effectively reduce the dimension of our data set and keep track of how much information we lose.  \n",
    "\n",
    "## **Using PCA to explore differences in populations**\n",
    "\n",
    "One of the main uses of PCA is to serve as a visual means to explore differences within a data set.  Unfortunately, it is very difficult for our minds to comprehend high dimensional data.  In order to interpret data, we need to transform it into 1-, 2- or 3-dimensional figures (you can also use time and colors, but interpretation gets more difficult).\n",
    "\n",
    "Let's see what our data looks like in just the first two principle components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "id": "L688HZdqq1ns",
    "outputId": "10b33e7f-55a1-49ef-c9f1-2d2b0de96932"
   },
   "outputs": [],
   "source": [
    "# Plotting our reduced representation based on the first 2 components\n",
    "components = [0,1]  # Pick which components to include.\n",
    "X_reduced = X @ V[components,:].T # Project data onto the\n",
    "\n",
    "# Plot the original and reduced data on two separate axes.\n",
    "ax0 = 0\n",
    "ax1 = 1\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X[:1000,ax0], X[:1000,ax1])\n",
    "plt.scatter(X[1000:,ax0], X[1000:,ax1])\n",
    "plt.xlabel(f'Variable {ax0}') \n",
    "plt.ylabel(f'Variable {ax1}')\n",
    "plt.legend(['Set 1','Set 2'])\n",
    "plt.title('Original Data')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X_reduced[:1000,0], X_reduced[:1000,1])\n",
    "plt.scatter(X_reduced[1000:,0], X_reduced[1000:,1])\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.legend(['Set 1','Set 2'])\n",
    "plt.title('Transformed Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXtV_vM6yZE1"
   },
   "source": [
    "Notice any trends? \n",
    "\n",
    "Try adjusting the axes (```ax0```, ```ax1```) of the original data to see if you could have done as well by just choosing which axes to plot. Some will do better than others, but it is a lot of work.\n",
    "\n",
    "Using the first few PCA vectors, it is much easier to distinguish between the two data types. Later in the course we will explore this for classification of different types of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlwS6h_e3FJv"
   },
   "source": [
    "## **Questions and practice**\n",
    "You should now be able to complete problems 12 and 13 in Homework 2, and also try the following on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNHquo6D3Ebz"
   },
   "outputs": [],
   "source": [
    "## Apply a scaling affine transformation matrix to a constellation of points.  This transformation should\n",
    "## scale (i.e. strectch or compress) the points in the x and y directions relative to some spatially fixed \n",
    "## origin point. \n",
    "## Hint: [ Scaling x   0       -rx ]\n",
    "##       [ 0       Scaling y   -ry ]\n",
    "##       [ 0           0         1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fyVm_kXO27C0"
   },
   "outputs": [],
   "source": [
    "## Apply SVD to an image, then reconstruct the image with varying amounts of singular values\n",
    "## What do you notice about using more singular values for reconstruction vs less?\n",
    "## How does the \"quality\" of the reconstructed image relate to the explained variance?\n",
    "## Hint: You may have to apply SVD to each channel if you are using a H x W x Color image."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "054083162da645f9b247be27173efe1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3658851275694ee1a15f9d5a305c1e22",
      "max": 35624,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3e7a113c1fa34f8b9810b782447b3e8c",
      "value": 35624
     }
    },
    "0fb04728349c4dde94aaf9897492743f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_80c5f442014946df8957cc48dcca541e",
      "placeholder": "​",
      "style": "IPY_MODEL_c6017ca79bae42bfa03eb72781ab031f",
      "value": "100%"
     }
    },
    "1b8264c7ea5a421584e4871c2d8062ea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c63e0a213d2422fad1013a4c7dc40c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cee19a8eb4454b238b640ea1f7669a2f",
      "placeholder": "​",
      "style": "IPY_MODEL_7abc07cf6e164036be85a891356d0a79",
      "value": " 35624/35624 [00:40&lt;00:00, 870.16it/s]"
     }
    },
    "3658851275694ee1a15f9d5a305c1e22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e7a113c1fa34f8b9810b782447b3e8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7abc07cf6e164036be85a891356d0a79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "80c5f442014946df8957cc48dcca541e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8fa5a4cd197c4a19b703b9e3178d546e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0fb04728349c4dde94aaf9897492743f",
       "IPY_MODEL_054083162da645f9b247be27173efe1e",
       "IPY_MODEL_1c63e0a213d2422fad1013a4c7dc40c6"
      ],
      "layout": "IPY_MODEL_1b8264c7ea5a421584e4871c2d8062ea"
     }
    },
    "c6017ca79bae42bfa03eb72781ab031f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cee19a8eb4454b238b640ea1f7669a2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
